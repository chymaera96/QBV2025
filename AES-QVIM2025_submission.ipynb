{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QVIM-AES Submission Template\n",
    "\n",
    "This is the submission template for the Query by Vocal Imitation challenge at the 2025 AES International Conference on Artificial Intelligence and Machine Learning for Audio.\n",
    "\n",
    "The content of this notebook is inspired by the template provided by the task organizers of the [Sound Scene Synthesis Taks of the DCASE Challenge 2024](https://dcase.community/challenge2024/task-sound-scene-synthesis).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>Confidentiality Statement</b><br> As the organizers of this contest, we assure all participants that their submitted models and code will be treated with strict confidentiality. Submissions will only be accessed by the designated review team for evaluation purposes and will not be shared, distributed, or used beyond the scope of this challenge. Participants retain full ownership of their work. We will not claim any rights over the submitted materials, nor will we use them for any purpose outside of the challenge evaluation process. We appreciate your participation in this challenge.\n",
    "</div>\n",
    "\n",
    "#### How to create your submission\n",
    "- Get familiar with the existing code blocks and the example provided below.\n",
    "- Set the root path of your environment and your dataset below (\"TODO: DEFINE YOUR PATHS HERE.\").\n",
    "- Set up your project (\"TODO: SETUP YOUR PROJECT HERE.\").\n",
    "- Implement the retrieval interface below (\"TODO: ADD YOUR IMPLEMENTATION HERE.\").\n",
    "    - Use the provided helper functions (helpers) to download your source code, model checkpoints, etc.\n",
    "- Instantiate your retrieval model (\"TODO: INSTANTIATE YOUR MODEL HERE.\").\n",
    "- Before **submitting your notebook**, run this notebook in a clean conda environment (with python >= 3.10) on Ubuntu 24.04 and make sure the evaluation results are in line with your previous results.\n",
    "- Submit your notebooks and the technical report as described on our [website](https://qvim-aes.github.io/).\n",
    "\n",
    "##### Some Rules\n",
    "- DO NOT modify the other code cells.\n",
    "- DO NOT add new cells.\n",
    "- Store your project WITHIN 'ROOT_PATH' and your data within 'DATA_PATH'.\n",
    "- DO NOT use 'ROOT_PATH/output' folder; this is where we will store things.\n",
    "- DO NOT change the working directory (e.g., `os.chdir('/path/to/a/dir/that/does/not/exist/on/my/machine')`).\n",
    "- DO NOT use system commands (`!cd ~` or `os.system('cd ~')`, etc.) other than the ones used to set up your environment (i.e., install required packages with pip, conda, ...).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> \n",
    "Participant who submit malicious code will be disqualified.\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "# Install basic packages for template notebook.\n",
    "! pip install librosa numpy pandas tqdm GitPython gdown==5.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "# some imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNASsBThlM2s"
   },
   "source": [
    "## Description of the Retrieval Interface \n",
    "`QVIMModel` is the interface specification for all query by vocal imitation systems. Each submitted system is expected to subclass this interface and implement the `compute_similarities` method, which computes the similarities between all pairwise combinations of queries (vocal imitations) and items (reference sounds).\n",
    "\n",
    "`compute_similarities` takes two dictionaries as input:\n",
    "- queries is a dictionary mapping ids of items to be retrieved to the corresponding file paths.\n",
    "- items is a dictionary mapping query ids to the corresponding file paths\n",
    "\n",
    "Participants are expected to load the sounds themselves, e.g., with `librosa.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "008R-IAWX0C5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class QVIMModel(ABC):\n",
    "    @abstractmethod\n",
    "    def compute_similarities(\n",
    "        self, items: dict[str, str], queries: dict[str, str]\n",
    "    ) -> dict[str, dict[str, float]]:\n",
    "        \"\"\"Compute similarity scores between items to be retrieved and a set of queries.\n",
    "\n",
    "        Each <query, item> pairing should be assigned a single floating point score, where higher\n",
    "        scores indicate higher similarity.\n",
    "\n",
    "        Args:\n",
    "            items (dict[str, str]): A dictionary mapping ids of items to be retrieved to the corresponding file path\n",
    "            queries (dict[str, str]): A dictionary mapping query ids to the corresponding file path\n",
    "\n",
    "        Returns:\n",
    "            scores (dict[str, dict[str, float]]): A dictionary mapping query ids to a dictionary of item\n",
    "                ids and their corresponding similarity scores. E.g:\n",
    "                {\n",
    "                    \"query_1\": {\n",
    "                        \"item_1\": 0.8,\n",
    "                        \"item_2\": 0.6,\n",
    "                        ...\n",
    "                    },\n",
    "                    \"query_2\": {\n",
    "                        \"item_1\": 0.4,\n",
    "                        \"item_2\": 0.9,\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVKQD14BnoSd"
   },
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`helpers.py` contains some helpful functions for downloading code and model checkpoints from Google Drive, Git and public links.\n",
    "\n",
    "The functions were taken (with slight modifications) from the submission template provided by the task organizers of [Task 7 of the DCASE Challenge 2024: Sound Scene Synthesis](https://dcase.community/challenge2024/task-sound-scene-synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.902769Z",
     "start_time": "2025-04-05T01:07:57.799703Z"
    }
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "from helpers import (\n",
    "    google_drive_download,\n",
    "    wget_download,\n",
    "    git_clone_checkout,\n",
    "    unpack_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup your paths\n",
    "\n",
    "- Define `ROOT_PATH`; this is where your project lives; for testing, we'll replace it with our custom ROOT_PATH. We recommend using the current working directory ('.').\n",
    "- Define `DATA_PATH`; this is where your public development data lives; for testing, we'll replace it with our custom DATA_PATH. We recommend using 'data/qvim-dev'.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.910317Z",
     "start_time": "2025-04-05T01:07:57.908291Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: DEFINE YOUR PATHS HERE.\n",
    "\"\"\"\n",
    "\n",
    "# replace this with your custom ROOT_PATH; this is where your code/ checkpoints will be downloaded to\n",
    "ROOT_PATH = \".\"\n",
    "\n",
    "# path to the evaluation data; can be in ROOT_PATH\n",
    "DATA_PATH = os.path.join(\"data\", \"qvim-dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.962831Z",
     "start_time": "2025-04-05T01:07:57.959787Z"
    }
   },
   "outputs": [],
   "source": [
    "helpers.ROOT_PATH = ROOT_PATH\n",
    "os.makedirs(ROOT_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "sys.path.append(os.path.join(ROOT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup your environment, download checkpoints, etc.\n",
    "\n",
    "Setup your project and install the required packages here.\n",
    "The easiest way is to:\n",
    "1) convert your implementation into a package,\n",
    "2) clone the repository and checkout the specific branch and commit,\n",
    "3) install your package with pip install -e name_of_your_fancy_package\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Make sure your link to the repository and other URLs are publicly available.\n",
    "- Use **shared public URLs** (e.g. a shared Google Drive, Dropbox, Zenodo link) to download checkpoints into `ROOT_PATH`.\n",
    "- Use the provided helper functions (`google_drive_download`, `wget_download`, `git_clone_checkout`, and `unpack_file`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FA2V1lYC3fy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: SETUP YOUR PROJECT HERE.\n",
    "\"\"\"\n",
    "! pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 scikit-learn==1.6.1 matplotlib lightning pandas tokenizers nnAudio librosa>=0.10.0 einops einops-exts sentencepiece notebook wandb audiomentations \"numpy<2\" torchopenl3 -e ./CED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Implement the QVIMModel Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: ADD YOUR IMPLEMENTATION HERE.\n",
    "\"\"\"\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ced_model.feature_extraction_ced import (\n",
    "    CedFeatureExtractor as HFCedFeatureExtractor,\n",
    ")\n",
    "from ced_model.modeling_ced import CedForAudioClassification\n",
    "\n",
    "\n",
    "class MLPProjection(nn.Module):\n",
    "    \"\"\"MLP projection layer to match the one used in training\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "\n",
    "        # Add hidden layers if they exist\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_dim = h_dim\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.projection = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class CEDProjectionModel(QVIMModel):\n",
    "    def __init__(self, model_name=\"mispeech/ced-base\", projection_path=None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.sample_rate = 16000  # CED expects 16kHz audio\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load the feature extractor and model\n",
    "        self.feature_extractor = HFCedFeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = CedForAudioClassification.from_pretrained(model_name).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # Set model to eval mode and freeze parameters\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Load MLP projection\n",
    "        self.mlp_projection = None\n",
    "        if projection_path and os.path.exists(projection_path):\n",
    "            self.mlp_projection = self._load_pretrained_projection(projection_path)\n",
    "\n",
    "    def _load_pretrained_projection(self, model_path):\n",
    "        \"\"\"Load the MLP projection from a local .pt file\"\"\"\n",
    "        try:\n",
    "            # Hard-coded configuration for the specific model\n",
    "            input_dim = 768  # CED-base dimension\n",
    "            hidden_dims = []  # Empty hidden dims\n",
    "            output_dim = 256  # projection output dimension\n",
    "            dropout_rate = 0.0\n",
    "\n",
    "            # Create MLP with correct architecture\n",
    "            mlp = MLPProjection(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "\n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "            # Get the state dict and handle key renaming if needed\n",
    "            if \"model_state_dict\" in checkpoint:\n",
    "                state_dict = checkpoint[\"model_state_dict\"]\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "\n",
    "            # Check if keys need renaming from 'mlp.*' to 'projection.*'\n",
    "            original_keys = list(state_dict.keys())\n",
    "            if any(key.startswith(\"mlp.\") for key in original_keys):\n",
    "                new_state_dict = {}\n",
    "                for key, value in state_dict.items():\n",
    "                    if key.startswith(\"mlp.\"):\n",
    "                        new_key = key.replace(\"mlp.\", \"projection.\")\n",
    "                        new_state_dict[new_key] = value\n",
    "                    else:\n",
    "                        new_state_dict[key] = value\n",
    "                state_dict = new_state_dict\n",
    "\n",
    "            # Load the state dict\n",
    "            mlp.load_state_dict(state_dict)\n",
    "            mlp.eval()\n",
    "            mlp.to(self.device)\n",
    "\n",
    "            for param in mlp.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            print(f\"Successfully loaded pretrained MLP projection from {model_path}\")\n",
    "            return mlp\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pretrained MLP projection: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(self, audio_paths):\n",
    "        \"\"\"Extract CED features and apply projection\"\"\"\n",
    "        ced_features_list = []\n",
    "\n",
    "        for file_path in audio_paths:\n",
    "            try:\n",
    "                audio, orig_sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "\n",
    "                # Ensure minimum length for CED\n",
    "                if len(audio) < self.sample_rate / 4:\n",
    "                    audio = np.pad(\n",
    "                        audio,\n",
    "                        (0, int((self.sample_rate / 4)) - len(audio)),\n",
    "                        mode=\"constant\",\n",
    "                    )\n",
    "\n",
    "                # Use feature extractor\n",
    "                inputs = self.feature_extractor(\n",
    "                    audio,\n",
    "                    sampling_rate=self.sample_rate,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "                # Move inputs to device\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                # Get model outputs\n",
    "                with torch.no_grad():\n",
    "                    encoder_outputs = self.model.encoder(**inputs)\n",
    "                    last_hidden_state = encoder_outputs[\"logits\"][-1]\n",
    "                    # Global average pooling over the sequence dimension\n",
    "                    pooled_features = last_hidden_state.mean(dim=0).squeeze(0)\n",
    "                    ced_features_list.append(pooled_features)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                # Use zero embedding as fallback\n",
    "                if ced_features_list:\n",
    "                    zero_embedding = torch.zeros_like(ced_features_list[0])\n",
    "                else:\n",
    "                    zero_embedding = torch.zeros(768, device=self.device)\n",
    "                ced_features_list.append(zero_embedding)\n",
    "\n",
    "        # Stack CED features\n",
    "        ced_features_batch = torch.stack(ced_features_list)\n",
    "\n",
    "        # Apply MLP projection\n",
    "        if self.mlp_projection is not None:\n",
    "            ced_features_batch = self.mlp_projection(ced_features_batch)\n",
    "\n",
    "        return ced_features_batch\n",
    "\n",
    "    def compute_similarities(self, items, queries):\n",
    "        \"\"\"\n",
    "        Compute similarities between queries and items.\n",
    "\n",
    "        Args:\n",
    "            items (dict): Dictionary of item file paths.\n",
    "            queries (dict): Dictionary of query file paths.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing similarities for each query.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract features for items\n",
    "        item_features = {}\n",
    "        print(\"Processing items...\")\n",
    "        for item_id, file_path in tqdm(items.items(), total=len(items)):\n",
    "            feature = self.extract_features([file_path])\n",
    "            # Handle batch dimension\n",
    "            if feature.ndim == 2 and feature.shape[0] == 1:\n",
    "                processed_feature = feature.squeeze(0)\n",
    "            else:\n",
    "                processed_feature = feature.squeeze() if feature.ndim > 1 else feature\n",
    "\n",
    "            item_features[item_id] = processed_feature.detach().cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Extract features for queries\n",
    "        query_features = {}\n",
    "        print(\"Processing queries...\")\n",
    "        for query_id, file_path in tqdm(queries.items(), total=len(queries)):\n",
    "            feature = self.extract_features([file_path])\n",
    "            # Handle batch dimension\n",
    "            if feature.ndim == 2 and feature.shape[0] == 1:\n",
    "                processed_feature = feature.squeeze(0)\n",
    "            else:\n",
    "                processed_feature = feature.squeeze() if feature.ndim > 1 else feature\n",
    "\n",
    "            query_features[query_id] = processed_feature.detach().cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Compute similarities\n",
    "        results = {}\n",
    "        print(\"Computing similarities...\")\n",
    "        for query_id, query_feature in tqdm(query_features.items()):\n",
    "            results[query_id] = {}\n",
    "            # Normalize query feature for cosine similarity\n",
    "            query_feature_normalized = query_feature / query_feature.norm(\n",
    "                dim=-1, keepdim=True\n",
    "            )\n",
    "\n",
    "            for item_id, item_feature in item_features.items():\n",
    "                # Normalize item feature for cosine similarity\n",
    "                item_feature_normalized = item_feature / item_feature.norm(\n",
    "                    dim=-1, keepdim=True\n",
    "                )\n",
    "\n",
    "                similarity = torch.matmul(\n",
    "                    query_feature_normalized, item_feature_normalized.T\n",
    "                ).item()\n",
    "                results[query_id][item_id] = similarity\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create an Instance of your QVIMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: INSTANTIATE YOUR MODEL HERE.\n",
    "\"\"\"\n",
    "QBVIM_MODEL = CEDProjectionModel(\n",
    "    model_name=\"mispeech/ced-base\",\n",
    "    projection_path=os.path.join(ROOT_PATH, \"SC_ced-base_aug-both_proj-256.pt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dICiJ1tOm4Vh"
   },
   "source": [
    "## Create Predictions\n",
    "\n",
    "To run this, download the development dataset and store them in `DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flFJzBKtX2cw"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "items_path = os.path.join(DATA_PATH, \"Items\")\n",
    "item_files = pd.DataFrame(\n",
    "    {\"path\": list(glob(os.path.join(items_path, \"**\", \"*.wav\"), recursive=True))}\n",
    ")\n",
    "item_files[\"Class\"] = item_files[\"path\"].transform(lambda x: x.split(os.path.sep)[-2])\n",
    "item_files[\"Items\"] = item_files[\"path\"].transform(lambda x: x.split(os.path.sep)[-1])\n",
    "\n",
    "queries_path = os.path.join(DATA_PATH, \"Queries\")\n",
    "query_files = pd.DataFrame(\n",
    "    {\"path\": list(glob(os.path.join(queries_path, \"**\", \"*.wav\"), recursive=True))}\n",
    ")\n",
    "query_files[\"Class\"] = query_files[\"path\"].transform(lambda x: x.split(os.path.sep)[-2])\n",
    "query_files[\"Query\"] = query_files[\"path\"].transform(lambda x: x.split(os.path.sep)[-1])\n",
    "\n",
    "print(\"Total item files:\", len(item_files))\n",
    "print(\"Total query files:\", len(query_files))\n",
    "\n",
    "if len(query_files) == 0 or len(item_files) == 0:\n",
    "    raise ValueError(\n",
    "        \"No query files found! Download the development dataset and store it in 'DATA_PATH'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "scores = QBVIM_MODEL.compute_similarities(\n",
    "    items={row[\"Items\"]: row[\"path\"] for i, row in item_files.iterrows()},\n",
    "    queries={row[\"Query\"]: row[\"path\"] for i, row in query_files.iterrows()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:13.229554Z",
     "start_time": "2025-04-05T01:11:13.087740Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "os.makedirs(os.path.join(ROOT_PATH, \"output\"), exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"output\", \"similarities.json\"), \"w\") as f:\n",
    "    json.dump(scores, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the Public Development Set\n",
    "\n",
    "Computes the Reciprocal Rank (RR) for each query in the public development set. The RR is the inverted rank $r_i$ of the correct item for query $i$. Submissions will be ranked via the Mean Reciprocal Randk (MRR) of queries $Q$ on a hidden test set:\n",
    "\n",
    "$$MRR = \\frac{1}{\\lvert Q \\rvert} \\sum_{i=1}^{\\lvert Q\\rvert} \\frac{1}{r_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"output\", \"similarities.json\"), \"r\") as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "rankings = pd.DataFrame(\n",
    "    dict(\n",
    "        **{\"id\": [i for i in list(scores.keys())]},\n",
    "        **{\n",
    "            k: [v[k] for v in scores.values()]\n",
    "            for k in scores[list(scores.keys())[0]].keys()\n",
    "        },\n",
    "    )\n",
    ").set_index(\"id\")\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"DEV Dataset.csv\"), skiprows=1)[\n",
    "    [\"Label\", \"Class\", \"Items\", \"Query 1\", \"Query 2\", \"Query 3\"]\n",
    "]\n",
    "\n",
    "df = df.melt(\n",
    "    id_vars=[col for col in df.columns if \"Query\" not in col],\n",
    "    value_vars=[\"Query 1\", \"Query 2\", \"Query 3\"],\n",
    "    var_name=\"Query Type\",\n",
    "    value_name=\"Query\",\n",
    ").dropna()\n",
    "\n",
    "# remove missing files\n",
    "rankings = rankings.loc[df[\"Query\"].unique(), df[\"Items\"].unique()]\n",
    "\n",
    "# load file with ground truth, i.e., query->item mapping; column 0 is item, colum 1 query\n",
    "ground_truth = {row[\"Query\"]: [row[\"Items\"]] for i, row in df.iterrows()}\n",
    "\n",
    "# find the rank of the correct item (real recording) for each query (imitation)\n",
    "position_of_correct = {}\n",
    "missing_query_files = []\n",
    "for query, correct_item_list in ground_truth.items():\n",
    "    # Skip if query is not in the DataFrame\n",
    "    if query not in rankings.index:\n",
    "        missing_query_files.append(query)\n",
    "        continue\n",
    "    # Get row and sort items by similarity in descending order\n",
    "    sorted_items = rankings.loc[query].sort_values(ascending=False)\n",
    "    # Find rank of correct items\n",
    "    position_of_correct[query] = {\n",
    "        item: sorted_items.index.get_loc(item)\n",
    "        for item in correct_item_list\n",
    "        if item in sorted_items.index\n",
    "    }\n",
    "    assert len(position_of_correct[query]) == len(correct_item_list), (\n",
    "        f\"Missing item! Got: {list(position_of_correct[query].keys())}. Expected: {correct_item_list}\"\n",
    "    )\n",
    "\n",
    "# compute MRR\n",
    "normalized_rrs = []\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    rr, irr = [], []  # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        rr.append(1 / (rank + 1))\n",
    "        irr.append(1 / (i + 1))\n",
    "    normalized_rrs.append(sum(rr) / sum(irr))  # normalize MRR with ideal one\n",
    "mrr = np.mean(normalized_rrs)\n",
    "\n",
    "print(\"Missing query files: \", len(missing_query_files))\n",
    "print(\"Missing item files: \", missing_query_files)\n",
    "print(\"MRR random:\", round((1 / np.arange(1, len(df[\"Items\"].unique()))).mean(), 4))\n",
    "print(\"MRR       :\", round(mrr, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "ground_truth = {\n",
    "    row[\"Query\"]: [\n",
    "        row_[\"Items\"]\n",
    "        for j, row_ in df.drop_duplicates(\"Items\").iterrows()\n",
    "        if row_[\"Class\"] == row[\"Class\"]\n",
    "    ]\n",
    "    for i, row in df.drop_duplicates(\"Query\").iterrows()\n",
    "}\n",
    "\n",
    "position_of_correct = {}\n",
    "missing_query_files = []\n",
    "for query, correct_item_list in ground_truth.items():\n",
    "    # Skip if query is not in the DataFrame\n",
    "    if query not in rankings.index:\n",
    "        missing_query_files.append(query)\n",
    "        continue\n",
    "    # Get row and sort items by similarity in descending order\n",
    "    sorted_items = rankings.loc[query].sort_values(ascending=False)\n",
    "    # Find rank of correct items\n",
    "    position_of_correct[query] = {\n",
    "        item: sorted_items.index.get_loc(item)\n",
    "        for item in correct_item_list\n",
    "        if item in sorted_items.index\n",
    "    }\n",
    "    assert len(position_of_correct[query]) == len(correct_item_list), f\"Missing item!\"\n",
    "\n",
    "# compute MRR\n",
    "normalized_rrs = []\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    rr, irr = [], []  # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        rr.append(1 / (rank + 1))\n",
    "        irr.append(1 / (i + 1))\n",
    "    normalized_rrs.append(sum(rr) / sum(irr))  # normalize MRR with ideal one\n",
    "mrr = np.mean(normalized_rrs)\n",
    "\n",
    "# compute NDCG\n",
    "normalized_dcg = []\n",
    "ndcgs = {}\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    dcg, idcg = [], []  # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        dcg.append(1 / np.log2(rank + 2))\n",
    "        idcg.append(1 / np.log2(i + 2))\n",
    "    normalized_dcg.append(sum(dcg) / sum(idcg))  # normalize MRR with ideal one\n",
    "    ndcgs[query] = sum(dcg) / sum(idcg)\n",
    "ndcg = np.mean(normalized_dcg)\n",
    "\n",
    "print(\"Class-wise MRR :\", round(mrr, 4))\n",
    "print(\"Class-wise NDCG:\", round(ndcg, 4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
